{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc510e02",
   "metadata": {},
   "source": [
    "# Activity Recognition on Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ec603",
   "metadata": {},
   "source": [
    "This notebook contains commands that train and deploy a model on Sagemaker for the activity recognition project mentioned in the README of this repository. In the below cells, we take the most accurate model from the ActivityRecognition.ipynb notebook and deploy it in a Sagemaker environment. \n",
    "\n",
    "With the massive increase in data collection and availability, the days of developing a machine learning model on a local Jupyter notebook are numbered. For this reason, ML engineers and data scientists will be looking to leverage the cloud for running their workloads. \n",
    "\n",
    "Training and deploying our model in the cloud has several benefits. First, we can take advantage of AWS' on-demand resources and pay-as-you-go model. We only have to provision resources while we need them for either training or inference and do not have to pay for those resources while we are not using them. AWS provides many options for instance types, including both CPU and GPU instances. Instead of having to procure expensive hardware, we can run our workload on these instances and only pay for the amount of time that we are running our workload. Another benefit of moving our ML workload to the cloud is that we can experiment quickly. With access to virtually unlimited hardware and the ability to quickly spin up instances and other resources, the cloud is a good fit for most ML jobs. The benefits are more easily seen with training jobs that are compute-intensive. Our simple project may not benefit from moving to the cloud as much as training a distributed neural network on several GB's of data, for example. However, this notebook provides a starting point for moving your local ML workloads to the cloud and demonstrates my ability to leverage AWS for machine learning. \n",
    "\n",
    "There are three main options for using Sagemaker - use a built-in algorithm, use a pre-built container (script mode), or bring-your-own container. Since we want to use some of the most common ML frameworks while still writing our own custom logic, we will use the a pre-built container. We will first aggregate our code from our local notebook into a single Python script. Sagemaker uses Amazon Simple Storage Service (S3) as its datasource so we will need to upload our data to S3 using the AWS SDK. We will then use the applicable framework container provided by Sagemaker to run our script and pull our training data from S3. Once the model is trained, we will deploy it to an inference endpoint, where we can send new data points and obtain a prediction. Our notebook will also include a step to cleanup our environment (delete our endpoint) so that we are not paying for unnecessary resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "102520a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec48911",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'n_estimators': 163, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_depth': 25} # hyperparameters found in our local testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a814a",
   "metadata": {},
   "source": [
    "Note: the Sagemaker-specific code in the below script is derived from [this](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-script-mode/scikitlearn_script/train_deploy_scikitlearn_without_dependencies.py) Sagemaker example script. It has been changed to accomodate my particular dataset format and preprocessing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d1e9811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity_recognition_random_forest_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile activity_recognition_random_forest_script.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "import json\n",
    "import traceback\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker_training import environment\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse arguments.\n",
    "    \"\"\"\n",
    "    env = environment.Environment()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument(\"--max-depth\", type=int, default=10)\n",
    "    parser.add_argument(\"--n-jobs\", type=int, default=env.num_cpus)\n",
    "    parser.add_argument(\"--min-samples-split\", type=int, default=2)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=2)\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=120)\n",
    "\n",
    "    # data directories\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\")) # we will not be specifying a train channel because we have one set of input files so we will do the splitting ourselves (see below)\n",
    "\n",
    "    # model directory: we will use the default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Load entire dataset.\n",
    "    \"\"\"\n",
    "    header = ['seq_num', 'x_accel', 'y_accel', 'z_accel', 'activity']\n",
    "    \n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    files = [os.path.join(path, file) for file in os.listdir(path) if file.endswith(\"csv\")]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(\"Invalid # of files in dir: {}\".format(path))\n",
    "\n",
    "    raw_data = [pd.read_csv(file, header=None, names=header, index_col='seq_num') for file in files]\n",
    "    data = pd.concat(raw_data, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # drop rows where the activity == 0\n",
    "    rowsToDrop = data[data.activity == 0].index\n",
    "    data.drop(index=rowsToDrop, inplace=True, axis=0)\n",
    "\n",
    "    # labels are in the last column\n",
    "    y = data.iloc[:, -1]\n",
    "    X = data.iloc[:, :-1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=12)\n",
    "    \n",
    "    # TODO: add SMOTE (and add imbalanced-learn library as a dependency)\n",
    "    #     smote = SMOTE()\n",
    "    #     X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def start(args):\n",
    "    \"\"\"\n",
    "    Train a Random Forest Regressor\n",
    "    \"\"\"\n",
    "    print(\"Training mode\")\n",
    "\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = load_dataset(args.train)\n",
    "\n",
    "        hyperparameters = {\n",
    "            \"max_depth\": args.max_depth,\n",
    "            \"verbose\": 1,  # show all logs\n",
    "            \"min_samples_split\": args.min_samples_split,\n",
    "            \"n_estimators\": args.n_estimators,\n",
    "            \"min_samples_leaf\": args.min_samples_leaf\n",
    "        }\n",
    "        print(\"Training the classifier\")\n",
    "        model = RandomForestClassifier()\n",
    "        model.set_params(**hyperparameters)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Score: {}\".format(model.score(X_test, y_test)))\n",
    "        joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failureReason in the\n",
    "        # DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, \"failure\"), \"w\") as s:\n",
    "            s.write(\"Exception during training: \" + str(e) + \"\\\\n\" + trc)\n",
    "\n",
    "        # Printing this causes the exception to be in the training job logs, as well.\n",
    "        print(\"Exception during training: \" + str(e) + \"\\\\n\" + trc, file=sys.stderr)\n",
    "\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialized and return fitted model\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args, _ = parse_args()\n",
    "\n",
    "    start(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86a44c",
   "metadata": {},
   "source": [
    "To prevent embedding the name of my S3 bucket, we will use local mode to train our model and use files from our instance's attached volume. In production, we would most likely have our data in S3 and provision a separate instance for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fa8d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04a8f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Data/'\n",
    "train_instance_type = 'local'\n",
    "inputs = {'train': f'file://{train_dir}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e98985c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b009d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SKLearn(entry_point = 'activity_recognition_random_forest_script.py',\n",
    "                   framework_version = '0.23-1',\n",
    "                   py_version = 'py3',\n",
    "                   instance_type = train_instance_type,\n",
    "                   instance_count = 1,\n",
    "                   hyperparameters = hyperparameters,\n",
    "                   role = role,\n",
    "                   base_job_name = 'randomforest_script_mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcf1e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sklearn_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge',endpoint_name='randomforestregressor-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e73030a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m6wrbvkmhkc-algo-1-jwk0o |\u001b[0m [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\r\n",
      "\u001b[36m6wrbvkmhkc-algo-1-jwk0o |\u001b[0m [Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    0.0s finished\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m6wrbvkmhkc-algo-1-jwk0o |\u001b[0m 172.18.0.1 - - [28/Sep/2021:14:27:43 +0000] \"POST /invocations HTTP/1.1\" 200 136 \"-\" \"python-urllib3/1.26.6\"\r\n"
     ]
    }
   ],
   "source": [
    "# provide a data sample (activity should be 7 - Talking while Standing)\n",
    "test_features = np.array([2028, 2382, 2012]).reshape(1, -1)\n",
    "\n",
    "sklearn_predictor.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f39e73",
   "metadata": {},
   "source": [
    "As you can see, our endpoint returned 7. This was the expected class for the given x acceleration, y acceleration, and z acceleration values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06a0d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "sklearn_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
